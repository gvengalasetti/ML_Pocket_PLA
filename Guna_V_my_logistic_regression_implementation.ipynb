{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gvengalasetti/ML_Pocket_PLA/blob/main/Guna_V_my_logistic_regression_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "k4q_iSzpPAqa"
      },
      "outputs": [],
      "source": [
        "# Starter code for implementing logistic regressio as a Scikit-Learn Estimator\n",
        "# Mini-batch Gradient descent is used to find the minimal loss\n",
        "# Reference: https://scikit-learn.org/stable/developers/develop.html\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.metrics import euclidean_distances\n",
        "class MyMiniBatchLogisticRegression(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, max_iter=100,  tol=0.0001, batch_size=32, random_state=None):\n",
        "        self.max_iter_ = max_iter\n",
        "        self.tol_=tol\n",
        "        self.random_state_ = random_state\n",
        "        self.batch_size = batch_size\n",
        "        self.w_ = []\n",
        "    def fit(self, X, y):\n",
        "        # Check that X and y have correct shape\n",
        "        X, y = check_X_y(X, y)\n",
        "        # Store the classes seen during fit\n",
        "        self.classes_ = unique_labels(y)\n",
        "        # Make sure the labels are either 0 or 1\n",
        "        self._checkBinaryLabels()\n",
        "        self.X_ = X\n",
        "        self.y_ = y\n",
        "        # Return the classifier\n",
        "        self.w_=self._logisticRegression(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Check if fit has been called\n",
        "        check_is_fitted(self)\n",
        "        # Input validation\n",
        "        X = check_array(X)\n",
        "        y = []\n",
        "\n",
        "        probas = MyMiniBatchLogisticRegression._calc_probas(X, self.w_)\n",
        "        return [1 if p >= 0.5 else 0 for p in probas]\n",
        "\n",
        "    def _logisticRegression(self, X, y):\n",
        "      #Instruction 1\n",
        "#computer the gradient\n",
        "# g = Change in E intake(w(t)\n",
        "#move in direction v = -g\n",
        "# update the weights w(t+1)=w+nv^t\n",
        "#iterate through\n",
        "#return final weights\n",
        "\n",
        "#instruction 2\n",
        "#(sigmoid function/logistic function) f(z)=1/1+e^-z - where z is the linear combination of input features and input weights\n",
        "#    - converts into value between 0 and 1, to use for probability\n",
        "#hypothesis function hθ(x) = f(θ^T * x) = 1 / (1 + e^(-θ^T * x))\n",
        "#    - goal is to find 0 such that h0 accurately gives you most correct probability\n",
        "#cross-entropy loss (loss function) - J(θ) = -[y * log(hθ(x)) + (1 — y) * log(1 — hθ(x))]\n",
        "#    - goal is to measure how well logistic function performs, allows to adjust it\n",
        "#minimize loss function using gradient descent - θj := θj — α * ∂J(θ) / ∂θj\n",
        "#    - minimize loss, updates the weight based on the gradient of the loss function\n",
        "\n",
        "# Args:\n",
        "#     X - Feature matrix (mini-batch), shape (m, n)\n",
        "#     y - Labels for mini-batch, shape (m,)\n",
        "#     theta - Model parameters, shape (n,)\n",
        "#     learning_rate - Learning rate for gradient descent\n",
        "\n",
        "#     Returns:\n",
        "#     w paramters?\n",
        "#     b parameters?\n",
        "\n",
        "        rand = np.random.RandomState(self.random_state_)\n",
        "        w = rand.randn(X.shape[1]+1)\n",
        "        b=0.1\n",
        "        for iteration in range(self.max_iter_):\n",
        "          m = X.shape[0]  # Number of rows in mxn,\n",
        "          batch_indices = np.random.choice(m, self.batch_size, replace=False)  # No replacement\n",
        "          X_batch = X[batch_indices, :]# Mini-Batch Data\n",
        "          y_batch = y[batch_indices]\n",
        "\n",
        "\n",
        "          sig=MyMiniBatchLogisticRegression._calc_probas(X_batch, w)#predictions\n",
        "          loss = -(y_batch*np.log(sig)+(1-y_batch)*np.log(1-sig))\n",
        "          J = np.mean(loss)  # Mean loss over the mini-batch\n",
        "          X_with_bias = np.column_stack([np.ones(X_batch.shape[0]), X_batch])\n",
        "\n",
        "\n",
        "          # Compute the gradient of the loss with respect to weights\n",
        "          gradient_w = (1 / m) * np.dot(X_with_bias.T, (sig - y_batch))\n",
        "          gradient_b = (1 / m) * np.sum(sig - y_batch)\n",
        "\n",
        "          w = w - self.tol_ * gradient_w\n",
        "          b = b - self.tol_ * gradient_b\n",
        "\n",
        "          if iteration % 100 == 0:  # Print every 100 iterations\n",
        "          # Compute the cost (J) for monitoring the progress\n",
        "            cost = -(1 / m) * np.sum(y_batch * np.log(sig) + (1 - y_batch) * np.log(1 - sig))\n",
        "            print(f\"Iteration {iteration}, Cost: {cost:.4f}, Weights: {w}, Bias: {b}\")\n",
        "        print(\"\\nFinal weights:\", w)\n",
        "        print(\"Final bias:\", b)\n",
        "        return w\n",
        "         #\n",
        "        # mini-batch gradient descent logistric regression learning implementation\n",
        "        # please respect max_iter, tol, random_state, and batch_size.\n",
        "        #^^^^^\n",
        "    @staticmethod\n",
        "    def _calc_sigmoid (w, X):\n",
        "      z = np.dot(X, w) + b\n",
        "      b=0.1\n",
        "      sigmoid = 1 / (1 + np.exp(-z))\n",
        "      return sigmoid\n",
        "\n",
        "    @staticmethod\n",
        "    def _calc_probas(X, w):\n",
        "        D = np.array([(MyMiniBatchLogisticRegression._dotWithBias(x,w)) for x in X])\n",
        "        return np.array([MyMiniBatchLogisticRegression._sigmoid(d)  for d in D])\n",
        "\n",
        "    @staticmethod\n",
        "    def _sigmoid(v):\n",
        "        return 1/(1+np.exp(-v))\n",
        "\n",
        "    @staticmethod\n",
        "    def _dotWithBias(x, w):\n",
        "      return np.dot(w, np.insert(x,0,1).transpose())\n",
        "\n",
        "    def _checkBinaryLabels(self):\n",
        "      self.classes_.sort()\n",
        "      print(self.classes_)\n",
        "      if (not ([0,1] == self.classes_.tolist())):\n",
        "        raise Exception(\"Binary labels 0 and 1 expected!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Driver code to use MyLogisticRegression and test its performance."
      ],
      "metadata": {
        "id": "R7ssOgw5VrYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "SETOSA_URL_ = \"/content/drive/My Drive/CMPE257-Shared/iris-setosa-labels.csv\"\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "data = pd.read_csv(SETOSA_URL_, header = None)\n",
        "print(data.shape)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "setosa_data = data.drop(data.columns[4], axis=1)\n",
        "print(setosa_data.shape)\n",
        "print(setosa_data)\n",
        "\n",
        "setosa_labels = data[data.columns[4]]\n",
        "print(setosa_labels.shape)\n",
        "print(setosa_labels)\n",
        "\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(setosa_data, setosa_labels, random_state=1)\n",
        "\n",
        "print(train_data.shape)\n",
        "print(train_labels.shape)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model = MyMiniBatchLogisticRegression(batch_size = 64, random_state = 36)\n",
        "model.fit(train_data, train_labels)\n",
        "\n",
        "\n",
        "train_predicts = model.predict(train_data)\n",
        "train_score = accuracy_score(train_predicts, train_labels)\n",
        "print(\"train accuracy: \", train_score)\n",
        "\n",
        "test_predicts = model.predict(test_data)\n",
        "test_score = accuracy_score(test_predicts, test_labels)\n",
        "print(\"test accuracy: \", test_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ej0nBZHYVzJz",
        "outputId": "6d87c8cd-a0a9-4abc-9539-e4f59a0eccd1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "(100, 5)\n",
            "(100, 4)\n",
            "      0    1    2    3\n",
            "0   5.1  3.5  1.4  0.2\n",
            "1   4.9  3.0  1.4  0.2\n",
            "2   4.7  3.2  1.3  0.2\n",
            "3   4.6  3.1  1.5  0.2\n",
            "4   5.0  3.6  1.4  0.2\n",
            "..  ...  ...  ...  ...\n",
            "95  5.7  3.0  4.2  1.2\n",
            "96  5.7  2.9  4.2  1.3\n",
            "97  6.2  2.9  4.3  1.3\n",
            "98  5.1  2.5  3.0  1.1\n",
            "99  5.7  2.8  4.1  1.3\n",
            "\n",
            "[100 rows x 4 columns]\n",
            "(100,)\n",
            "0     1\n",
            "1     1\n",
            "2     1\n",
            "3     1\n",
            "4     1\n",
            "     ..\n",
            "95    0\n",
            "96    0\n",
            "97    0\n",
            "98    0\n",
            "99    0\n",
            "Name: 4, Length: 100, dtype: int64\n",
            "(75, 4)\n",
            "(75,)\n",
            "[0 1]\n",
            "Iteration 0, Cost: 5.4250, Weights: [ 0.67636928  1.5208394  -0.51199806  1.15000826 -0.59077534], Bias: 0.09995601679291514\n",
            "\n",
            "Final weights: [ 0.67243917  1.49751284 -0.52290656  1.13315963 -0.59608367]\n",
            "Final bias: 0.09602589936113251\n",
            "train accuracy:  0.5333333333333333\n",
            "test accuracy:  0.4\n"
          ]
        }
      ]
    }
  ]
}